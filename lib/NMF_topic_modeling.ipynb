{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cleaning import clean_doc\n",
    "import numpy as np\n",
    "\n",
    "from nlp import NLPPipe\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV of the articles \n",
    "articles = pd.read_csv('../src/clean_text.csv', index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   article_id        date                                               text\n",
       "0       42343  2020-09-03  data aggregation is the process of gathering d...\n",
       "1       42356  2020-09-04  there are thousand of dataset repository on th...\n",
       "2       42424  2020-09-05  one key operation in preparing the datasets in...\n",
       "3       42431  2020-09-05  this is a follow up of my introduction to the ...\n",
       "4       42445  2020-09-06  structured query language sql is famously know..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>42343</td>\n      <td>2020-09-03</td>\n      <td>data aggregation is the process of gathering d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42356</td>\n      <td>2020-09-04</td>\n      <td>there are thousand of dataset repository on th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>42424</td>\n      <td>2020-09-05</td>\n      <td>one key operation in preparing the datasets in...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42431</td>\n      <td>2020-09-05</td>\n      <td>this is a follow up of my introduction to the ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>42445</td>\n      <td>2020-09-06</td>\n      <td>structured query language sql is famously know...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "words_to_add = ['file', 'python', 'code','wa', 'people','need', 'model', 'models']\n",
    "for word in words_to_add:\n",
    "    stop_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = NLPPipe(\n",
    "    cleaning_function=clean_doc,\n",
    "    vectorizer=vectorizer,\n",
    "    model=nmf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pipe.vectorizer.fit_transform(articles['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = pipe.model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pipe.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pipe('../models/nmf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = NLPPipe()\n",
    "test.load_pipe(filename='../models/nmf_model.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "157001"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "len(test.vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTopic  0\ntime, one, would, like, thing, get, work, could, way, know, think, even, make, something, day, might, really, want, problem, much, good, lot, go, question, going, take, look, see, say, many\n\nTopic  1\nimage, images, pixel, convolution, color, convolutional, style, cnn, filter, augmentation, vision, face, layer, size, transfer, pooling, opencv, channel, segmentation, recognition, rgb, computer, vgg, trained, dog, label, cat, picture, folder, classification\n\nTopic  2\nlearning, machine, deep, algorithm, ml, learn, course, neural, supervised, book, computer, language, min, unsupervised, intelligence, artificial, problem, read, algorithms, task, knowledge, reinforcement, field, networks, programming, concept, research, library, network, vision\n\nTopic  3\napp, api, web, command, notebook, project, install, page, create, package, google, use, jupyter, run, click, html, script, py, folder, environment, text, using, github, server, directory, request, library, flask, url, git\n\nTopic  4\nword, text, sentence, vector, document, embeddings, nlp, language, words, embedding, sentiment, vec, corpus, sequence, idf, context, bert, topic, tweet, vocabulary, token, tf, frequency, character, representation, attention, natural, similarity, processing, vectors\n\nTopic  5\ndata, analysis, science, big, analytics, business, scientist, tool, set, information, quality, process, organization, source, visualization, insight, pipeline, cleaning, processing, missing, driven, datasets, use, often, collection, privacy, analyst, warehouse, scientists, ha\n\nTopic  6\nlayer, network, neural, input, output, activation, neuron, weight, function, hidden, gradient, loss, networks, layers, deep, convolutional, convolution, architecture, relu, sigmoid, lstm, keras, pooling, batch, cnn, sequence, propagation, forward, backpropagation, size\n\nTopic  7\ntraining, validation, dataset, train, test, set, accuracy, data, trained, tensorflow, batch, loss, cross, performance, parameter, epoch, keras, split, hyperparameters, pytorch, using, use, overfitting, learning, prediction, gpu, label, tuning, testing, fold\n\nTopic  8\nai, intelligence, human, artificial, technology, system, company, ml, world, business, industry, ha, robot, research, google, bias, automation, future, decision, computer, development, intelligent, today, humans, machine, brain, new, digital, healthcare, society\n\nTopic  9\nagent, action, reward, state, policy, reinforcement, environment, rl, value, game, learning, episode, actions, optimal, algorithm, function, update, rewards, dqn, step, current, greedy, states, bellman, markov, td, transition, actor, agents, critic\n\nTopic  10\ncustomer, business, product, company, analytics, marketing, churn, customers, team, service, sale, purchase, revenue, organization, cost, market, decision, offer, transaction, help, strategy, technology, management, consumer, insight, campaign, predictive, analysis, brand, segment\n\nTopic  11\ncluster, clustering, means, distance, clusters, algorithm, point, centroid, number, group, unsupervised, silhouette, elbow, data, dataset, dbscan, points, method, euclidean, hierarchical, similarity, spark, centroids, similar, kmeans, nearest, two, different, density, optimal\n\nTopic  12\ndistribution, probability, bayesian, random, normal, likelihood, posterior, event, prior, sample, bayes, gaussian, mean, theorem, coin, parameter, variable, function, given, value, estimate, uncertainty, number, sampling, deviation, conditional, standard, density, entropy, binomial\n\nTopic  13\nplot, chart, visualization, bar, plotly, matplotlib, color, map, axis, country, line, scatter, data, dataset, plotting, seaborn, variable, graph, plots, show, interactive, histogram, figure, library, charts, dashboard, see, create, tableau, pie\n\nTopic  14\nregression, linear, function, gradient, variable, value, descent, logistic, error, equation, line, cost, coefficient, loss, squared, fit, parameter, derivative, algorithm, weight, slope, independent, point, dependent, variables, minimum, relationship, regularization, let, polynomial\n\nTopic  15\ntree, decision, forest, random, trees, node, split, algorithm, boosting, gini, leaf, impurity, ensemble, bagging, entropy, xgboost, prediction, classification, depth, sample, gain, learner, splitting, root, classifier, overfitting, forests, bias, search, branch\n\nTopic  16\nfeature, features, variable, categorical, dataset, selection, missing, importance, correlation, target, value, pca, score, encoding, engineering, column, variables, values, data, numerical, method, correlated, shap, prediction, important, dimensionality, forest, principal, technique, scikit\n\nTopic  17\nuser, item, movie, rating, recommendation, recommender, system, filtering, collaborative, users, product, similarity, based, content, song, items, movies, ratings, rated, book, factorization, matrix, netflix, genre, review, recommendations, interaction, similar, systems, music\n\nTopic  18\nprice, stock, series, time, market, forecast, forecasting, trading, arima, day, trend, average, stationary, period, financial, portfolio, lstm, seasonality, return, seasonal, prophet, year, prediction, date, lag, moving, month, predict, strategy, prices\n\nTopic  19\nmatrix, vector, array, pca, numpy, dimension, dimensional, space, covariance, eigenvectors, variance, principal, element, linear, eigenvalue, component, vectors, algebra, row, multiplication, kernel, matrices, svd, product, dot, dimensionality, distance, equation, diagonal, two\n\nTopic  20\ncolumn, function, dataframe, pandas, value, row, list, index, method, string, use, let, missing, panda, columns, array, name, return, values, df, using, dictionary, frame, object, numpy, element, want, dataset, data, argument\n\nTopic  21\nobject, detection, box, bounding, yolo, class, cnn, segmentation, video, car, yolov, map, face, vision, mask, detector, region, objects, frame, detect, method, coco, camera, computer, anchor, rcnn, boxes, network, proposal, ssd\n\nTopic  22\nnode, graph, edge, nodes, network, vertex, graphs, centrality, neo, connected, structure, edges, adjacency, path, community, relationship, entity, connection, shortest, degree, undirected, gnn, tensorflow, knowledge, networkx, directed, social, representation, list, networks\n\nTopic  23\ndocker, container, spark, aws, cloud, run, service, kubernetes, ml, command, pipeline, dockerfile, sagemaker, environment, running, instance, airflow, lambda, server, application, deploy, deployment, apache, azure, port, production, amazon, gpu, containers, software\n\nTopic  24\nplayer, game, team, season, players, win, nba, league, play, games, score, played, winning, match, stats, top, teams, roi, shot, per, football, pts, opponent, fantasy, playing, goal, point, position, average, week\n\nTopic  25\nclass, positive, recall, precision, classification, false, score, negative, classifier, roc, metric, accuracy, curve, confusion, auc, true, threshold, predicted, imbalanced, logistic, tp, sentiment, prediction, probability, label, classes, binary, positives, minority, spam\n\nTopic  26\nhypothesis, test, sample, null, population, value, mean, testing, statistical, significance, statistic, error, reject, group, difference, confidence, size, variance, distribution, variable, standard, deviation, effect, experiment, significant, correlation, interval, two, true, level\n\nTopic  27\nscience, scientist, job, skill, data, project, company, interview, team, role, career, business, work, experience, technical, skills, scientists, field, engineer, programming, learn, question, course, engineering, software, knowledge, industry, resume, working, projects\n\nTopic  28\ndiscriminator, generator, gan, gans, fake, loss, adversarial, generative, network, training, real, generated, paper, generate, image, latent, output, noise, networks, function, input, sample, encoder, dcgan, vector, decoder, images, architecture, distribution, generation\n\nTopic  29\nsql, query, table, database, bigquery, join, mysql, relational, tables, select, row, queries, schema, clause, column, postgresql, databases, statement, language, id, server, db, spark, sqlite, name, record, create, data, bi, use\n"
     ]
    }
   ],
   "source": [
    "display_topics(pipe.model, vocab, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None, show_weights=False):\n",
    "    \"\"\"\n",
    "    Displays Top words associated with topics from Topic Modeling\n",
    "\n",
    "    model: trained NLP Model (SVD, NMF)\n",
    "    feature_names: feature names from vectorizers\n",
    "    no_top_words: number of words to show\n",
    "    topic_names: List of topic names to assign topics\n",
    "    show_weights: True to show weights of important words. \n",
    "    \"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        if show_weights:\n",
    "            print([(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        \n",
    "        else:\n",
    "            print(\", \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [np.argsort(row) for row in pipe.model.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_mat = pd.DataFrame(topic_results, columns=[f'Topic {str(i)}' for i in range(30)], index=articles.article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Topic 0   Topic 1   Topic 2   Topic 3   Topic 4   Topic 5  \\\n",
       "article_id                                                               \n",
       "42343       0.000000  0.000000  0.000000  0.000000  0.000000  0.026794   \n",
       "42356       0.000000  0.001041  0.000000  0.017656  0.000000  0.013377   \n",
       "42424       0.000000  0.000000  0.000000  0.000000  0.000000  0.000127   \n",
       "42431       0.003051  0.000000  0.000000  0.000000  0.000000  0.013785   \n",
       "42445       0.002229  0.000000  0.000000  0.000000  0.000000  0.020910   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "42291       0.006118  0.000000  0.000000  0.001568  0.000000  0.003258   \n",
       "42303       0.008548  0.000000  0.000000  0.000000  0.000416  0.008736   \n",
       "42315       0.004919  0.000000  0.000000  0.000000  0.001852  0.035587   \n",
       "42324       0.000000  0.000000  0.000000  0.014829  0.036062  0.000000   \n",
       "42331       0.008770  0.000000  0.000954  0.004780  0.001945  0.000000   \n",
       "\n",
       "             Topic 6   Topic 7   Topic 8   Topic 9  ...  Topic 20  Topic 21  \\\n",
       "article_id                                          ...                       \n",
       "42343       0.000000  0.000000  0.000000  0.000000  ...  0.037588  0.000000   \n",
       "42356       0.000000  0.034122  0.009723  0.000000  ...  0.000000  0.000000   \n",
       "42424       0.000000  0.000000  0.000000  0.000000  ...  0.124587  0.050166   \n",
       "42431       0.000443  0.000000  0.000000  0.000000  ...  0.011021  0.000000   \n",
       "42445       0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "42291       0.003031  0.000000  0.000000  0.003085  ...  0.003591  0.001500   \n",
       "42303       0.005694  0.002180  0.000680  0.000000  ...  0.000648  0.002010   \n",
       "42315       0.000000  0.000700  0.000000  0.000000  ...  0.000000  0.016961   \n",
       "42324       0.000000  0.000000  0.000000  0.000000  ...  0.006374  0.007678   \n",
       "42331       0.000000  0.000000  0.000000  0.000000  ...  0.043720  0.000000   \n",
       "\n",
       "            Topic 22  Topic 23  Topic 24  Topic 25  Topic 26  Topic 27  \\\n",
       "article_id                                                               \n",
       "42343       0.000000  0.000000  0.000000  0.000000  0.019837  0.000000   \n",
       "42356       0.000486  0.000000  0.000000  0.000000  0.000000  0.015989   \n",
       "42424       0.000000  0.000000  0.000000  0.000000  0.000000  0.002118   \n",
       "42431       0.000281  0.012307  0.000000  0.000000  0.000000  0.000000   \n",
       "42445       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "42291       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "42303       0.001475  0.000000  0.001695  0.000380  0.000000  0.000000   \n",
       "42315       0.000000  0.000000  0.009622  0.018686  0.026147  0.006165   \n",
       "42324       0.000000  0.002436  0.000000  0.000000  0.000000  0.000000   \n",
       "42331       0.003387  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            Topic 28  Topic 29  \n",
       "article_id                      \n",
       "42343       0.000000  0.000000  \n",
       "42356       0.000000  0.007457  \n",
       "42424       0.000000  0.000000  \n",
       "42431       0.000000  0.010401  \n",
       "42445       0.000000  0.148953  \n",
       "...              ...       ...  \n",
       "42291       0.000000  0.000000  \n",
       "42303       0.000000  0.000524  \n",
       "42315       0.000000  0.000000  \n",
       "42324       0.000000  0.000000  \n",
       "42331       0.001507  0.000000  \n",
       "\n",
       "[35623 rows x 30 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic 0</th>\n      <th>Topic 1</th>\n      <th>Topic 2</th>\n      <th>Topic 3</th>\n      <th>Topic 4</th>\n      <th>Topic 5</th>\n      <th>Topic 6</th>\n      <th>Topic 7</th>\n      <th>Topic 8</th>\n      <th>Topic 9</th>\n      <th>...</th>\n      <th>Topic 20</th>\n      <th>Topic 21</th>\n      <th>Topic 22</th>\n      <th>Topic 23</th>\n      <th>Topic 24</th>\n      <th>Topic 25</th>\n      <th>Topic 26</th>\n      <th>Topic 27</th>\n      <th>Topic 28</th>\n      <th>Topic 29</th>\n    </tr>\n    <tr>\n      <th>article_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42343</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.026794</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.037588</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.019837</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42356</th>\n      <td>0.000000</td>\n      <td>0.001041</td>\n      <td>0.000000</td>\n      <td>0.017656</td>\n      <td>0.000000</td>\n      <td>0.013377</td>\n      <td>0.000000</td>\n      <td>0.034122</td>\n      <td>0.009723</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000486</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.015989</td>\n      <td>0.000000</td>\n      <td>0.007457</td>\n    </tr>\n    <tr>\n      <th>42424</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000127</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.124587</td>\n      <td>0.050166</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002118</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42431</th>\n      <td>0.003051</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013785</td>\n      <td>0.000443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.011021</td>\n      <td>0.000000</td>\n      <td>0.000281</td>\n      <td>0.012307</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010401</td>\n    </tr>\n    <tr>\n      <th>42445</th>\n      <td>0.002229</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.020910</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.148953</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42291</th>\n      <td>0.006118</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001568</td>\n      <td>0.000000</td>\n      <td>0.003258</td>\n      <td>0.003031</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.003085</td>\n      <td>...</td>\n      <td>0.003591</td>\n      <td>0.001500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42303</th>\n      <td>0.008548</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000416</td>\n      <td>0.008736</td>\n      <td>0.005694</td>\n      <td>0.002180</td>\n      <td>0.000680</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000648</td>\n      <td>0.002010</td>\n      <td>0.001475</td>\n      <td>0.000000</td>\n      <td>0.001695</td>\n      <td>0.000380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000524</td>\n    </tr>\n    <tr>\n      <th>42315</th>\n      <td>0.004919</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001852</td>\n      <td>0.035587</td>\n      <td>0.000000</td>\n      <td>0.000700</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.016961</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.009622</td>\n      <td>0.018686</td>\n      <td>0.026147</td>\n      <td>0.006165</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42324</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014829</td>\n      <td>0.036062</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.006374</td>\n      <td>0.007678</td>\n      <td>0.000000</td>\n      <td>0.002436</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>42331</th>\n      <td>0.008770</td>\n      <td>0.000000</td>\n      <td>0.000954</td>\n      <td>0.004780</td>\n      <td>0.001945</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.043720</td>\n      <td>0.000000</td>\n      <td>0.003387</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.001507</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>35623 rows Ã— 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "doc_topic_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({18: 1189,\n",
       "         7: 1273,\n",
       "         20: 1975,\n",
       "         5: 2344,\n",
       "         29: 1001,\n",
       "         21: 1177,\n",
       "         26: 1211,\n",
       "         12: 724,\n",
       "         16: 864,\n",
       "         27: 1744,\n",
       "         13: 1396,\n",
       "         2: 1542,\n",
       "         14: 1024,\n",
       "         22: 612,\n",
       "         9: 774,\n",
       "         17: 834,\n",
       "         1: 1187,\n",
       "         3: 2076,\n",
       "         23: 995,\n",
       "         4: 1997,\n",
       "         10: 687,\n",
       "         8: 1722,\n",
       "         19: 893,\n",
       "         6: 1730,\n",
       "         24: 787,\n",
       "         25: 741,\n",
       "         0: 1357,\n",
       "         28: 449,\n",
       "         11: 643,\n",
       "         15: 675})"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(np.argmax(topic_results, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.argmax(topic_results,axis=1), index=articles.article_id, columns=['topic']).reset_index().to_csv('../src/nmf_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}