{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('spark': conda)",
   "metadata": {
    "interpreter": {
     "hash": "258ac9dacf52a9f1ab5bf3e30bc7ee43d0d8d18db3d9056252a7bd0c013d9f1d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Preparation Notebook\n",
    "Author: Andrew Auyeung  \n",
    "This notebook is used to load the LDA model trained on google colab and export the \n",
    "document-topic-matrix as a CSV for visualizations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from pyspark.ml import PipelineModel\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcea8b3d580>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://andrews-mbp.fios-router.home:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Start Spark\n",
    "spark = pyspark.sql.SparkSession.builder.config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "lda_model  = PipelineModel.load('../models/lda_article_model')\n",
    "\n",
    "# Load CSV into spark\n",
    "articles = spark.read.csv('../src/TDS_articles.csv', inferSchema=True, header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vocab, topic_indexes, topic_labels=None):\n",
    "    \"\"\"\n",
    "    Helper function to show topics from the pyspark LDA results\n",
    "    args:\n",
    "        vocab (list): list of all words in the vectorized vocabulary\n",
    "        topic_indexes (list): indexes of words associated with topics from LDA\n",
    "        topic_labels (list): labeled topic names.  Naively matches topics with \n",
    "            number of topics. i.e. topic 1, topic 2... topic n\n",
    "    \"\"\"\n",
    "    if not topic_labels:\n",
    "        topic_labels = [f\"Topic: {str(i)}\" for i in range(len(topic_indexes))]\n",
    "        assert len(topic_labels) == len(topic_indexes)\n",
    "\n",
    "    for label, words in zip(topic_labels, topic_indexes):\n",
    "        topic_words = \", \".join([vocab[word_idx] for word_idx in words])\n",
    "        print(label + \" \" + topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = lda_model.stages[2].vocabulary\n",
    "topics = lda_model.stages[4].describeTopics().collect()\n",
    "topic_idx = [topic.termIndices for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 ai, customer, rating, marketing, city, human, technology, min, intelligence, patient\nTopic: 1 country, covid, loan, coronavirus, death, county, population, virus, gdp, rate\nTopic: 2 table, join, query, column, sql, dataframe, price, database, row, yard\nTopic: 3 inceptionv, yolov, deeplabv, fm, rcnn, resnet, multipathnet, stn, cvpr, resnext\nTopic: 4 image, file, model, function, layer, training, code, network, python, action\nTopic: 5 scraping, scrapy, selenium, espresso, scrape, soup, spider, html, web, page\nTopic: 6 player, game, season, team, league, players, nba, win, shot, football\nTopic: 7 song, music, automl, rasa, aws, spotify, artist, songs, iris, lyric\nTopic: 8 wine, minority, chromosome, imbalanced, smote, rfm, oversampling, ros, radar, customer\nTopic: 9 quantum, airflow, exam, election, vote, voter, centrality, dag, qubits, bigquery\nTopic: 10 word, text, sentence, document, vector, language, sentiment, nlp, sequence, embedding\nTopic: 11 agent, infected, arxiv, al, et, brain, deep, video, simulation, bike\nTopic: 12 science, business, ai, scientist, company, data, team, people, project, wa\nTopic: 13 forecast, seasonal, arima, forecasting, shap, series, shapley, lag, seasonality, stationary\nTopic: 14 model, feature, regression, distribution, variable, probability, function, value, x, network\nTopic: 15 bi, pi, raspberry, docker, federated, tableau, microservice, pigeon, subsequence, asd\nTopic: 16 earthquake, wine, bci, acidity, sulfur, molecule, dioxide, eeg, alcohol, bcis\nTopic: 17 spark, cluster, stock, aws, price, azure, portfolio, julia, apache, market\nTopic: 18 bob, alice, de, que, codeheres, lisp, encryption, tqdm, cipher, tax\nTopic: 19 protein, streamlit, amino, dna, acid, album, chart, dash, molecule, hexagon\n"
     ]
    }
   ],
   "source": [
    "show_topics(vocab, topic_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/andrew/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/andrew/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up cleaning to prep articles:\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "def remove_digits(text):\n",
    "    \"\"\"\n",
    "    Uses Regex to replace anything that is not an alphabetic character with an empty string\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "\n",
    "def lemma(text, pos_to_avoid=[]):\n",
    "    \"\"\"\n",
    "    Lemmatize Text.  Pass in a list of POS to avoid when lemmatizing\n",
    "    \"\"\"\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split()]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Remove emojis from text\n",
    "    \"\"\"\n",
    "    return re.sub(r':\\w*:', ' ', text)\n",
    "def remove_multiple_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove multiple consecutive spaces from text\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "def clean_doc(text):\n",
    "\n",
    "    # remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    # lemmatize and remove pronouns\n",
    "    text = lemma(text, ['-PRON-'])\n",
    "    # remove digits and punctuation\n",
    "    text = remove_digits(text)\n",
    "    # change all to lowercase\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = remove_multiple_spaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register cleaning function as UDF \n",
    "from pyspark.sql.functions import udf, split, explode, col, posexplode\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# add the cleaning function as a UDF\n",
    "clean_udf = udf(clean_doc)\n",
    "\n",
    "# make a UDF to remove the bracket delimiters\n",
    "remove_brackets = udf(lambda row: row.replace('}\"', '').replace('\"{\"', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------------------+--------------------+--------------------+----------+-----+------+----------+--------------------+--------------------+\n|article_id|               title|            subtitle|              author|      date|claps|images|codeblocks|                link|                body|\n+----------+--------------------+--------------------+--------------------+----------+-----+------+----------+--------------------+--------------------+\n|     42343|Mastering Data Ag...|Aggregating Data ...|Sadrach Pierre, P...|2020-09-03|   40|     8|         8|https://towardsda...|\"Data aggregation...|\n|     42356|1 trick to find a...|Search and Contri...|        Satyam Kumar|2020-09-04|    7|     4|         0|https://towardsda...|\"There are thousa...|\n|     42424|How to Extract Da...|Explore the power...|     Yong Cui, Ph.D.|2020-09-05|  115|     1|         0|https://towardsda...|\"One key operatio...|\n|     42431|Delta Lake in Act...|Beginnerâ€™s Guide ...|        Jyoti Dhiman|2020-09-05|   17|     1|        12|https://towardsda...|\"This is a follow...|\n|     42445|Logical Flow of S...|For all the data ...|   Manoj Bidadi Raju|2020-09-06|  106|     4|         0|https://towardsda...|\"Structured Query...|\n+----------+--------------------+--------------------+--------------------+----------+-----+------+----------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# apply remove brackets to body column\n",
    "articles = (articles\n",
    "  .withColumn('body', remove_brackets('body'))\n",
    "  )\n",
    "\n",
    "articles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+--------------------+\n|article_id|      date|                text|\n+----------+----------+--------------------+\n|     42343|2020-09-03|data aggregation ...|\n|     42356|2020-09-04|there are thousan...|\n|     42424|2020-09-05|one key operation...|\n|     42431|2020-09-05|this is a follow ...|\n|     42445|2020-09-06|structured query ...|\n+----------+----------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "train_corpa = (\n",
    "    articles\n",
    "        .select('article_id', 'date', 'body')\n",
    "        .where(col('body').isNotNull())\n",
    "        .withColumn('body', clean_udf('body'))\n",
    "        .withColumnRenamed('body', 'text')\n",
    "    )\n",
    "\n",
    "train_corpa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = articles.select('body').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(text, columns=['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['body'] = text['body'].map(lambda x: \" \".join(str(x).split('\",\"'))).map(lambda x: x[1:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv('../src/text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = train_corpa.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "clean_text = pd.DataFrame(clean_text, columns=['article_id', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_n = pd.read_csv(\"../src/TDS_document_topic_matrix_cleaned.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "35623"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "len(final_n['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text[clean_text['article_id'].isin(final_n['article_id'])].to_csv('../src/clean_text.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text.to_csv('../src/clean_text.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|article_id|      date|                text|              tokens|     filtered_tokens|           term_freq|            features|   topicDistribution|\n+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|      3406|2018-03-24|there is a theore...|[there, is, a, th...|[theorem, telling...|(262144,[0,1,2,4,...|(262144,[0,1,2,4,...|[6.17526639750102...|\n|      5405|2019-07-02|this article is a...|[this, article, i...|[article, case, k...|(262144,[0,1,2,4,...|(262144,[0,1,2,4,...|[0.09939988428045...|\n+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "doc_topic_mat = lda_model.transform(train_corpa)\n",
    "doc_topic_mat.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|article_id|      date|                text|Topic 0|Topic 1|Topic 2|Topic 3|Topic 4|Topic 5|Topic 6|Topic 7|Topic 8|Topic 9|Topic 10|Topic 11|Topic 12|Topic 13|Topic 14|Topic 15|Topic 16|Topic 17|Topic 18|Topic 19|\n+----------+----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|      3406|2018-03-24|there is a theore...|    0.0|    0.0|    0.0|    0.0|  0.162|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.191|     0.0|   0.629|     0.0|     0.0|     0.0|     0.0|   0.017|\n|      5405|2019-07-02|this article is a...|  0.099|    0.0|  0.017|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.852|   0.015|     0.0|     0.0|     0.0|   0.016|     0.0|     0.0|\n|      5957|2019-11-26|why are competent...|  0.153|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|   0.045|   0.802|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      6932|      null|this article wa o...|    0.0|    0.0|    0.0|    0.0|  0.256|  0.016|    0.0|    0.0|    0.0|    0.0|   0.554|     0.0|     0.0|     0.0|   0.173|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8041|2019-07-20|in a previous art...|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.065|     0.0|   0.935|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8060|2019-07-21|every now and aga...|    0.0|    0.0|  0.222|  0.019|  0.615|    0.0|    0.0|    0.0|    0.0|    0.0|   0.142|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8061|2019-07-21|when i first star...|    0.0|    0.0|  0.306|    0.0|  0.342|  0.143|    0.0|  0.015|    0.0|  0.056|     0.0|     0.0|   0.138|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8062|2019-07-21|are you bored of ...|    0.0|    0.0|    0.0|    0.0|  0.482|  0.257|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.091|     0.0|     0.0|     0.0|     0.0|   0.169|     0.0|     0.0|\n|      8063|2019-07-21|as a budding data...|    0.0|    0.0|    0.0|    0.0|  0.477|    0.0|    0.0|    0.0|    0.0|  0.026|     0.0|     0.0|   0.341|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|   0.156|\n|      8064|2019-07-21|next episode star...|  0.191|    0.0|    0.0|    0.0|  0.156|  0.003|  0.119|    0.0|    0.0|  0.098|   0.324|     0.0|     0.1|     0.0|     0.0|     0.0|   0.008|     0.0|     0.0|     0.0|\n|      8065|2019-07-21|stationarity is a...|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.015|   0.472|   0.512|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8066|2019-07-20|while i wa learni...|  0.223|    0.0|  0.511|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|   0.049|   0.148|   0.068|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8068|2019-07-21|a multilabel clas...|    0.0|    0.0|    0.0|    0.0|  0.173|    0.0|    0.0|    0.0|    0.0|    0.0|    0.23|     0.0|     0.0|     0.0|   0.544|     0.0|     0.0|    0.05|     0.0|     0.0|\n|      8069|2019-07-21|lets face it your...|    0.0|    0.0|    0.0|    0.0|  0.993|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|   0.007|     0.0|     0.0|\n|      8070|2019-07-21|recently graph ha...|  0.016|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|  0.042|    0.0|  0.031|   0.021|     0.0|     0.0|     0.0|   0.878|     0.0|   0.011|     0.0|     0.0|     0.0|\n|      8071|2019-07-21|this article is a...|  0.042|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|  0.138|     0.0|     0.0|    0.82|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8072|2019-07-21|deciphering fedsp...|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|   0.635|     0.0|    0.23|     0.0|   0.103|     0.0|     0.0|   0.032|     0.0|     0.0|\n|      8073|2019-07-21|data science is t...|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|  0.174|   0.39|     0.0|     0.0|     0.0|     0.0|   0.435|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8074|2019-07-21|in the match of b...|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|     0.0|   0.296|   0.703|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|\n|      8075|2019-07-21|a new task called...|  0.015|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|   0.908|     0.0|     0.0|     0.0|   0.076|     0.0|     0.0|     0.0|     0.0|     0.0|\n+----------+----------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import round\n",
    "# Save topic matrix as csv to export to tableau\n",
    "dtm = (doc_topic_mat\n",
    "    .withColumn('distribution_array', vector_to_array(doc_topic_mat['topicDistribution']))\n",
    "    .select(\n",
    "        ['article_id', 'date', 'text'] + \n",
    "        [round(col('distribution_array')[i], scale=3).alias(f\"Topic {str(i)}\") for i in range(20)])\n",
    "    )\n",
    "\n",
    "dtm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df = dtm.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   article_id        date                                               text  \\\n",
       "0        3406  2018-03-24  there is a theorem telling u there is no singl...   \n",
       "1        5405  2019-07-02  this article is a case for keeping retaining a...   \n",
       "2        5957  2019-11-26  why are competent people often so bad at teach...   \n",
       "3        6932        None  this article wa originally posted by derrick m...   \n",
       "4        8041  2019-07-20  in a previous article we discussed the concept...   \n",
       "\n",
       "   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  Topic 6  ...  \\\n",
       "0    0.000      0.0    0.000      0.0    0.162    0.000      0.0  ...   \n",
       "1    0.099      0.0    0.017      0.0    0.000    0.000      0.0  ...   \n",
       "2    0.153      0.0    0.000      0.0    0.000    0.000      0.0  ...   \n",
       "3    0.000      0.0    0.000      0.0    0.256    0.016      0.0  ...   \n",
       "4    0.000      0.0    0.000      0.0    0.000    0.000      0.0  ...   \n",
       "\n",
       "   Topic 10  Topic 11  Topic 12  Topic 13  Topic 14  Topic 15  Topic 16  \\\n",
       "0     0.000     0.000     0.191     0.000     0.629       0.0       0.0   \n",
       "1     0.000     0.000     0.852     0.015     0.000       0.0       0.0   \n",
       "2     0.000     0.045     0.802     0.000     0.000       0.0       0.0   \n",
       "3     0.554     0.000     0.000     0.000     0.173       0.0       0.0   \n",
       "4     0.000     0.000     0.065     0.000     0.935       0.0       0.0   \n",
       "\n",
       "   Topic 17  Topic 18  Topic 19  \n",
       "0     0.000       0.0     0.017  \n",
       "1     0.016       0.0     0.000  \n",
       "2     0.000       0.0     0.000  \n",
       "3     0.000       0.0     0.000  \n",
       "4     0.000       0.0     0.000  \n",
       "\n",
       "[5 rows x 23 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>date</th>\n      <th>text</th>\n      <th>Topic 0</th>\n      <th>Topic 1</th>\n      <th>Topic 2</th>\n      <th>Topic 3</th>\n      <th>Topic 4</th>\n      <th>Topic 5</th>\n      <th>Topic 6</th>\n      <th>...</th>\n      <th>Topic 10</th>\n      <th>Topic 11</th>\n      <th>Topic 12</th>\n      <th>Topic 13</th>\n      <th>Topic 14</th>\n      <th>Topic 15</th>\n      <th>Topic 16</th>\n      <th>Topic 17</th>\n      <th>Topic 18</th>\n      <th>Topic 19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3406</td>\n      <td>2018-03-24</td>\n      <td>there is a theorem telling u there is no singl...</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.162</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.191</td>\n      <td>0.000</td>\n      <td>0.629</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.017</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5405</td>\n      <td>2019-07-02</td>\n      <td>this article is a case for keeping retaining a...</td>\n      <td>0.099</td>\n      <td>0.0</td>\n      <td>0.017</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.852</td>\n      <td>0.015</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.016</td>\n      <td>0.0</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5957</td>\n      <td>2019-11-26</td>\n      <td>why are competent people often so bad at teach...</td>\n      <td>0.153</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.045</td>\n      <td>0.802</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6932</td>\n      <td>None</td>\n      <td>this article wa originally posted by derrick m...</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.256</td>\n      <td>0.016</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.554</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.173</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8041</td>\n      <td>2019-07-20</td>\n      <td>in a previous article we discussed the concept...</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.065</td>\n      <td>0.000</td>\n      <td>0.935</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 23 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(35646, 23)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "dtm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df.to_csv('../src/TDS_document_topic_matrix.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}